{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Function mapping using *adaptive* package\n",
    "In some cases the relation between the input variables and the output cannot be expressed analytically \n",
    "and only a numerical solution is possible. The process of obtaining an output for a wide range of input values and possibly creating an interpolation function is called function mapping. \n",
    "\n",
    "Considering that such relation can be presented in form of a callable function, a dedicated *adaptive* package provides a convenient way to perform this task. The package is based on the Bayesian optimization algorithm and is designed to work with functions that are expensive to evaluate. The package offers functionality to effectively map 1D and 2D functions and offers a possibility to extend it to higher dimensions. \n",
    "\n",
    "The function map can be used for creating an interpolation function for obtaining function values that were not calculated directly.\n",
    "\n",
    "In the following example, a typical usage routine of the *adaptive* package for mapping the function is demonstrated. It is directly applied to the Reaction-Diffusion Equation case to reveal relation between growth rate profile features and  process parameters $p_o$ and $\\tau_r$. Only 2D mapping (using Learner2D) is supported."
   ],
   "id": "a2573004bb0ac012"
  },
  {
   "cell_type": "code",
   "id": "71127bc8e2ca4c62",
   "metadata": {},
   "source": [
    "import os, time\n",
    "from math import sqrt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import adaptive\n",
    "import matplotlib.pyplot as plt\n",
    "from backend import adaptive_tools\n",
    "from backend.processclass2 import Experiment1D_Dimensionless\n",
    "\n",
    "%matplotlib notebook\n",
    "adaptive.notebook_extension()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. Define the function to be learned\n",
    "Here three various functions are defined to demonstrate the mapping process. They can be simply swapped to observe the mapping process live.\n"
   ],
   "id": "110372c6b96b80f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first one is a simple exponential function",
   "id": "c164651eb489a47c"
  },
  {
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "cell_type": "code",
   "source": [
    "def exp_func(xy):\n",
    "    time.sleep(0.05)\n",
    "    x, y = xy\n",
    "    return np.exp(x/2) - x / np.exp(y/5)"
   ],
   "id": "5515dd7a2cef22b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The second one is a super-Gaussian function",
   "id": "2b54b66ae504601e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def super_gauss(xy):\n",
    "    time.sleep(0.01)\n",
    "    x, y = xy\n",
    "    R = sqrt(x**2+y**2)\n",
    "    s = 2\n",
    "    n = 5\n",
    "    return 1/s/sqrt(2*np.pi) *np.exp(-(R**2/2/s**2)**n)"
   ],
   "id": "7eab65a5394fb6d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our target function is the relation between the indented growth rate profile peak position `r_max_n` and the process parameters `p_o` and `tau_r` that is obtained by solving the RDE equation of the Continuum model.\n",
    "Here we use the `Experiment1D_Dimensionless` class that represents the dimensionless version of the RDE equation to obtain the solution.\n",
    "First, we need to set up the RDE solver with necessary parameters:"
   ],
   "id": "68e78eca29268bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pr = Experiment1D_Dimensionless()\n",
    "pr.beam_type = 'gauss'\n",
    "pr.f0 = 1e6\n",
    "pr.fwhm = 500\n",
    "pr.step = 2\n",
    "pr.order = 1"
   ],
   "id": "f1c72ec633912dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we define a simple function that can receive the input parameters, set them into the RDE solver and return the result:",
   "id": "a24b293f7ae2223a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rde_r_max(xy):\n",
    "    global pr\n",
    "    _ = Experiment1D_Dimensionless() # this is needed to initialize internal variables\n",
    "    pr = deepcopy(pr)\n",
    "    x, y = xy\n",
    "    pr.p_o = x\n",
    "    pr.tau_r = y\n",
    "    pr.solve_steady_state()\n",
    "    return pr.r_max_n"
   ],
   "id": "609570f25b18f769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Define the domain of the function\n",
    "\n",
    "Define the extent of the domain for each variable that the fucntion will be learned in.\n",
    "\n",
    "Here the $p_o$ is associated with x-axis and is limited between 0 and 20.\n",
    "\n",
    "The $\\tau_r$ is associated with y-axis and is limited between 1.0001 and 10000."
   ],
   "id": "540857e7c0192578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bounds = ((1e-6, 3), (1.0001, 2000))",
   "id": "458d456c5d3b069e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Learn the function\n",
    "\n",
    "The learning process is started as soon as the `runner` object is created. It will show learning progress, number of points evaluated and current loss value.\n",
    "\n",
    "The criteria used to finish the learning process is the *loss goal*. \n",
    "The smaller the value, the more accurate is the learned function. However, the smaller the value, the more time is needed to learn the function.\n",
    "\n",
    "The number of tasks (engaged CPU cores) is limited to 8, by default all CPU cores are used."
   ],
   "id": "3755301a95d7954d"
  },
  {
   "cell_type": "code",
   "id": "dc5a71c86b63a2e",
   "metadata": {},
   "source": [
    "filename = 'test_mapping.int'\n",
    "# Create the learner and provide it the function to be learned and the domain limits (bounds)\n",
    "learner = adaptive.Learner2D(function=exp_func, bounds=bounds)\n",
    "# Launch the learning process\n",
    "runner = adaptive.Runner(learner, loss_goal=0.01, ntasks=4)\n",
    "# It is possible to enable periodic saving of the learning result\n",
    "# runner.start_periodic_saving(save_kwargs=dict(fname=filename), interval=60)\n",
    "runner.live_info()\n",
    "runner.live_plot(update_interval=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The result can be quicly plotted from the learner itself",
   "id": "a31935972a1b0417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "learner.plot(n=500, tri_alpha=0.2)",
   "id": "8dc65400e76f4c83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If any issues occur, the traceback can be displayed:",
   "id": "18d55e1322acc8f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for point, tb in runner.tracebacks:\n",
    "    print(f\"point: {point}:\\n {tb}\")\n"
   ],
   "id": "5d01db6a61cc9f88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Inspect the learned function\n",
    "\n",
    "Plot a large map of the learned function to see how well function features are captured.\n",
    "\n",
    "If the accuracy is not satisfactory, the learning process can be continued by lowering the loss goal value."
   ],
   "id": "3f921e39e92741dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot(learner, npoints=300, tri_alpha=0.2, width=300, height=300, xlim=None, ylim=None):\n",
    "    plot = learner.plot(npoints, tri_alpha=tri_alpha)\n",
    "    if xlim is not None:\n",
    "        plot.opts(xlim=xlim)\n",
    "    if ylim is not None:\n",
    "        plot.opts(ylim=ylim)\n",
    "    plot.opts(width=width, height=height)\n",
    "    return plot"
   ],
   "id": "f3be2b1db9d0553f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55a6ad35a1e169af",
   "metadata": {},
   "source": "plot(learner, npoints=1500, tri_alpha=0.2, width=800, height=800)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Evaluate the learned function\n",
    "\n",
    "Now the interpolation function is ready, it can be used to quickly assess the function value at any point in the domain without needing to solve the RDE.\n",
    "\n",
    "It can be done in two ways:\n",
    "* By directly creating an evenly spaced grid of x, y, z values just by specifying the number of points in a dimension:"
   ],
   "id": "afefef6a79cbbf3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x, y, z = learner.interpolated_on_grid(2000)",
   "id": "7bff686ef699fbf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* By extracting the interpolation function and specifying the x, y values manually:",
   "id": "f7f7062a7824f518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from backend.adaptive_tools import learner_interpolator\n",
    "\n",
    "interpolator = learner_interpolator(learner)\n",
    "\n",
    "xi = np.linspace(0, 3, 200)\n",
    "yi = np.linspace(1.0001, 2000, 200)\n",
    "xx, yy = np.meshgrid(xi, yi)\n",
    "zz = interpolator(xx, yy)"
   ],
   "id": "f5fffd272733acb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Keep in mind that the first method generates 1D arrays of x, y, while in the second method the xx and yy grids are 2D arrays.",
   "id": "a1808f5ce89655d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Save the learned function\n",
    "\n",
    "The learned function can be saved to a file and used later for interpolation."
   ],
   "id": "85e574b42ba48049"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "learner.save(filename)",
   "id": "e4dc10270b316a46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Load the learned function\n",
    "\n",
    "The saved learned function can be loaded from a file and used for interpolation."
   ],
   "id": "e67f7af4991cab37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from backend.adaptive_tools import learner_load_full\n",
    "\n",
    "learner_loaded = learner_load_full(filename)\n",
    "learner_loaded.plot(n=500, tri_alpha=0.2)\n",
    "interpolator = learner_interpolator(learner_loaded)"
   ],
   "id": "85c1492a85186294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Learning ina transformed space\n",
    "In some cases the learned function may have strong non-linear behavior (exponential, quadratic etc), whereas the learning algorithm chooses points based on the linear interpolation, making learning process inefficient. In such cases, it is beneficial to first transform the domain into a linear space, learn the function and then transform the learned function back to the original space."
   ],
   "id": "1f11f51271757113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Consider again the example with the RDE equation. By plotting the data from `examples\\r_max_interp_1.0.int` file it becomes evident that the function has high variability in the lower-x part of the domain and low variability in the upper part:",
   "id": "b131d7f3b07437a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname_example = r'../examples/r_max_interp_1.0.int'\n",
    "learner_example = adaptive_tools.learner_load_full(fname_example)\n",
    "learner_example.plot(n=500, tri_alpha=0.2)"
   ],
   "id": "4e48ce35a833b5a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Such behaviour prompts for a transformation of the x-axis. In this case, the logarithmic transformation is applied to the x-axis. The transformation is applied to the domain limits, the function itself and the interpolation function.\n",
    "To apply initial transformation to the logarithmic space, the bounds and the function need to be modified.\n",
    "The logarithm has to be applied to the x-bounds:"
   ],
   "id": "d2fbdda46be89602"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xmin = np.log10(bounds[0][0])\n",
    "xmax = np.log10(bounds[0][1])\n",
    "\n",
    "bounds_logx = ((xmin, xmax), bounds[1])\n",
    "bounds_logx"
   ],
   "id": "313a53501bf2d66e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the learner will be looking at the transformed logarithmic space when choosing the next point for evaluation.\n",
    "\n",
    "Next, the learner sends the x and y values to the function. Here the x value has to be transformed back before actual evaluation:"
   ],
   "id": "24bd31dade4ca7f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rde_r_max_logx(xy):\n",
    "    global pr\n",
    "    _ = Experiment1D_Dimensionless() # this is needed to initialize internal variables\n",
    "    pr = deepcopy(pr)\n",
    "    logx, y = xy\n",
    "    x = 10**logx\n",
    "    pr.p_o = x\n",
    "    pr.tau_r = y\n",
    "    pr.solve_steady_state()\n",
    "    return pr.r_max_n"
   ],
   "id": "498aa8ca78dbc5f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After that the learner can be started in the same manner as it ws shown above:",
   "id": "7da46b560a4cab94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filename = 'test_mapping_logx.int'\n",
    "# Create the learner and provide it the function to be learned and the domain limits (bounds)\n",
    "learner_logx = adaptive.Learner2D(function=exp_func, bounds=bounds_logx)\n",
    "# Launch the learning process\n",
    "runner_logx = adaptive.Runner(learner_logx, loss_goal=0.01, ntasks=1)\n",
    "# It is possible to enable periodic saving of the learning result\n",
    "# runner.start_periodic_saving(save_kwargs=dict(fname=filename), interval=60)\n",
    "runner_logx.live_info()\n",
    "runner_logx.live_plot(update_interval=0.5)"
   ],
   "id": "700e1b3b7831c5c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the learning process is finished, the interpolation function can be transformed back to the original space:",
   "id": "ffb5896b66a8c5be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = adaptive_tools.learner_data_to_numpy(learner_logx)\n",
    "x = 10**data[:, 0]\n",
    "data[:, 0] = x\n",
    "learner_logx.data = data\n",
    "learner_logx.bounds = bounds\n",
    "learner_logx.plot(n=500, tri_alpha=0.2)"
   ],
   "id": "e58ab7c84d030057",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
